"""
å›¾åƒç¼“å­˜ä¿å­˜èŠ‚ç‚¹ - Image Cache Save Node
å°†å›¾åƒæ•°æ®ç¼“å­˜åˆ°æŒ‡å®šé€šé“ï¼Œä¾›è·å–èŠ‚ç‚¹ä¸»åŠ¨è¯»å–
"""

import sys
import os
import time
import torch
import numpy as np
from PIL import Image
from typing import List, Dict, Any, Optional, Tuple
from ..image_cache_manager.image_cache_manager import cache_manager

# å¯¼å…¥debugé…ç½®
from ..utils.debug_config import debug_print

CATEGORY_TYPE = "danbooru"
COMPONENT_NAME = "image_cache_save"


class AnyType(str):
    """ç”¨äºè¡¨ç¤ºä»»æ„ç±»å‹çš„ç‰¹æ®Šç±»ï¼Œåœ¨ç±»å‹æ¯”è¾ƒæ—¶æ€»æ˜¯è¿”å›ç›¸ç­‰"""
    def __eq__(self, _) -> bool:
        return True

    def __ne__(self, __value: object) -> bool:
        return False


any_typ = AnyType("*")


# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹ - ä»å…±äº«æ¨¡å—å¯¼å…¥
# cache_manager = ImageCacheManager()


class ImageCache:
    """
    å›¾åƒç¼“å­˜ä¿å­˜èŠ‚ç‚¹ - å°†å›¾åƒç¼“å­˜åˆ°æŒ‡å®šé€šé“ä¾›å…¶ä»–èŠ‚ç‚¹è·å–
    """

    # ç±»çº§åˆ«çš„ç¼“å­˜ï¼šå­˜å‚¨æ£€æµ‹ç»“æœï¼Œæ‰€æœ‰å®ä¾‹å…±äº«
    _has_manager_cache = None  # Noneè¡¨ç¤ºæœªæ£€æµ‹ï¼ŒTrue/Falseè¡¨ç¤ºæ£€æµ‹ç»“æœ
    _warning_sent_cache = False  # ç±»çº§åˆ«çš„è­¦å‘Šå‘é€æ ‡å¿—

    def __init__(self):
        pass

    @classmethod
    def _check_for_group_executor_manager(cls, prompt: Optional[Dict]) -> bool:
        """
        æ£€æµ‹å·¥ä½œæµä¸­æ˜¯å¦æœ‰GroupExecutorManagerèŠ‚ç‚¹
        ä½¿ç”¨ç±»çº§åˆ«ç¼“å­˜ï¼Œåªåœ¨ç¬¬ä¸€æ¬¡æ‰§è¡Œæ—¶æ£€æµ‹

        Args:
            prompt: å·¥ä½œæµæ•°æ®

        Returns:
            bool: æ˜¯å¦å­˜åœ¨GroupExecutorManagerèŠ‚ç‚¹
        """
        # å¦‚æœå·²ç»ç¼“å­˜äº†æ£€æµ‹ç»“æœï¼Œç›´æ¥è¿”å›
        if cls._has_manager_cache is not None:
            return cls._has_manager_cache

        # ç¬¬ä¸€æ¬¡æ£€æµ‹
        if not prompt:
            cls._has_manager_cache = False
            return False

        try:
            # promptæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å·¥ä½œæµæ•°æ®å­—å…¸
            workflow_data = prompt[0] if isinstance(prompt, list) and len(prompt) > 0 else prompt

            # éå†å·¥ä½œæµä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
            if isinstance(workflow_data, dict):
                for node_id, node_data in workflow_data.items():
                    if isinstance(node_data, dict) and node_data.get("class_type") == "GroupExecutorManager":
                        print(f"[ImageCacheSave] âœ“ æ£€æµ‹åˆ°GroupExecutorManagerèŠ‚ç‚¹ï¼ˆå·²ç¼“å­˜ï¼‰")
                        cls._has_manager_cache = True
                        return True

            print(f"[ImageCacheSave] âš  æœªæ£€æµ‹åˆ°GroupExecutorManagerèŠ‚ç‚¹ï¼ˆå·²ç¼“å­˜ï¼‰")
            cls._has_manager_cache = False
            return False
        except Exception as e:
            print(f"[ImageCacheSave] æ£€æµ‹GroupExecutorManagerå¤±è´¥: {str(e)}")
            cls._has_manager_cache = False
            return False

    @classmethod
    def _send_no_manager_warning(cls):
        """å‘é€WebSocketè­¦å‘Šåˆ°å‰ç«¯ï¼ˆç±»çº§åˆ«ï¼Œåªå‘é€ä¸€æ¬¡ï¼‰"""
        if cls._warning_sent_cache:
            return  # é¿å…é‡å¤å‘é€

        try:
            from server import PromptServer
            PromptServer.instance.send_sync("cache-node-no-manager-warning", {
                "node_type": "ImageCacheSave",
                "message": "å›¾åƒç¼“å­˜èŠ‚ç‚¹éœ€è¦é…åˆç»„æ‰§è¡Œç®¡ç†å™¨ä½¿ç”¨ï¼Œè¯·æ·»åŠ ç»„æ‰§è¡Œç®¡ç†å™¨å’Œè§¦å‘å™¨ååˆ·æ–°ç½‘é¡µ\nImage cache nodes require Group Executor Manager. Please add Manager and Trigger, then refresh."
            })
            cls._warning_sent_cache = True
            print(f"[ImageCacheSave] å·²å‘é€WebSocketè­¦å‘Š")
        except ImportError:
            print("[ImageCacheSave] è­¦å‘Š: ä¸åœ¨ComfyUIç¯å¢ƒä¸­ï¼Œè·³è¿‡WebSocketé€šçŸ¥")
        except Exception as e:
            print(f"[ImageCacheSave] WebSocketè­¦å‘Šå‘é€å¤±è´¥: {e}")

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "images": ("IMAGE", {"tooltip": "è¦ç¼“å­˜çš„å›¾åƒ"}),
            },
            "optional": {
                "channel_name": ("STRING", {
                    "default": "default",
                    "tooltip": "ç¼“å­˜é€šé“åç§°ï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„å›¾åƒç¼“å­˜"
                }),
                "enable_preview": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ§åˆ¶æ˜¯å¦ç”Ÿæˆç¼“å­˜å›¾åƒçš„é¢„è§ˆ"
                })
            },
            "hidden": {"prompt": "PROMPT", "extra_pnginfo": "EXTRA_PNGINFO"},
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("pass_through",)
    FUNCTION = "cache_images"
    CATEGORY = CATEGORY_TYPE
    INPUT_IS_LIST = True
    OUTPUT_NODE = True
    DESCRIPTION = "å°†å›¾åƒç¼“å­˜åˆ°æŒ‡å®šé€šé“ï¼Œå¹¶è¾“å‡ºåŸå§‹å›¾åƒä¾›ä¸‹æ¸¸èŠ‚ç‚¹ä½¿ç”¨ï¼ˆæ§åˆ¶æ‰§è¡Œé¡ºåºï¼‰"

    def cache_images(self,
                    images: List,
                    prompt: Optional[Dict] = None,
                    extra_pnginfo: Optional[Dict] = None,
                    channel_name: List[str] = ["default"],
                    enable_preview: List[bool] = [True]) -> Dict[str, Any]:
        """
        å°†å›¾åƒç¼“å­˜åˆ°æŒ‡å®šé€šé“ï¼ˆæ”¯æŒå¤šé€šé“éš”ç¦»ï¼‰

        åŠŸèƒ½ï¼š
        - æ”¯æŒè‡ªå®šä¹‰é€šé“åç§°
        - ä¿å­˜å‰æ¸…ç©ºæŒ‡å®šé€šé“çš„ç¼“å­˜
        - æ‰§è¡Œé¡ºåºç”±GroupExecutorManageræ§åˆ¶
        """
        try:
            timestamp = time.strftime("%H:%M:%S", time.localtime())
            debug_print(COMPONENT_NAME, f"\n{'='*60}")
            debug_print(COMPONENT_NAME, f"[ImageCacheSave] â° æ‰§è¡Œæ—¶é—´: {timestamp}")
            debug_print(COMPONENT_NAME, f"[ImageCacheSave] ğŸ” å½“å‰ç»„å: {cache_manager.current_group_name}")
            debug_print(COMPONENT_NAME, f"[ImageCacheSave] â”Œâ”€ å¼€å§‹ä¿å­˜å›¾åƒ")
            debug_print(COMPONENT_NAME, f"{'='*60}\n")

            # âœ… æ£€æµ‹å·¥ä½œæµä¸­æ˜¯å¦æœ‰GroupExecutorManagerèŠ‚ç‚¹ï¼ˆä½¿ç”¨ç¼“å­˜ï¼Œåªæ£€æµ‹ä¸€æ¬¡ï¼‰
            has_manager = ImageCache._check_for_group_executor_manager(prompt)
            if not has_manager:
                # å‘é€WebSocketäº‹ä»¶åˆ°å‰ç«¯æ˜¾ç¤ºtoastï¼ˆåªå‘é€ä¸€æ¬¡ï¼‰
                ImageCache._send_no_manager_warning()

            # å‚æ•°å¤„ç† - ç¡®ä¿æ­£ç¡®æå–å‚æ•°å€¼ï¼ˆINPUT_IS_LIST=Trueï¼‰
            processed_channel_name = channel_name[0] if isinstance(channel_name, list) and len(channel_name) > 0 else "default"
            if not processed_channel_name or processed_channel_name.strip() == "":
                processed_channel_name = "default"

            processed_enable_preview = enable_preview[0] if isinstance(enable_preview, list) else enable_preview

            debug_print(COMPONENT_NAME, f"[ImageCacheSave] ğŸ“ é€šé“: {processed_channel_name}")

            # âœ… ä¿å­˜å‰æ¸…ç©ºæŒ‡å®šé€šé“çš„ç¼“å­˜
            debug_print(COMPONENT_NAME, f"[ImageCacheSave] ğŸ—‘ï¸ æ¸…ç©ºé€šé“ '{processed_channel_name}' çš„ç¼“å­˜")
            cache_manager.clear_cache(channel_name=processed_channel_name)

            # å°†è¾“å…¥çš„æ‰¹æ¬¡åˆ—è¡¨å±•å¼€ä¸ºå•ä¸ªå›¾åƒå¼ é‡åˆ—è¡¨
            # ComfyUI's INPUT_IS_LIST=True wraps inputs in a list.
            # Each item in the list is a batch tensor (B, H, W, C).
            # We need to unpack all batches into a single flat list of images for the manager.
            unpacked_images = []
            for batch in images:
                unpacked_images.extend(list(batch))  # Iterating over a tensor unpacks the first dimension

            # ä¿å­˜åˆ°æŒ‡å®šé€šé“
            results = cache_manager.cache_images(
                images=unpacked_images,
                filename_prefix="cached_image",
                preview_rgba=True,
                clear_before_save=False,  # å·²ç»åœ¨ä¸Šé¢æ¸…ç©ºè¿‡äº†
                channel_name=processed_channel_name
            )

            debug_print(COMPONENT_NAME, f"[ImageCacheSave] â””â”€ ä¿å­˜å®Œæˆ: {len(results)} å¼ ")

            # è¿”å›åŸå§‹å›¾åƒä¾›ä¸‹æ¸¸èŠ‚ç‚¹ä½¿ç”¨ï¼ˆå¼ºåˆ¶æ‰§è¡Œé¡ºåºï¼‰
            if unpacked_images:
                # å°†æ‰€æœ‰å›¾åƒtensorå †å æˆä¸€ä¸ªæ‰¹æ¬¡
                if len(unpacked_images) > 1:
                    output_batch = torch.stack(unpacked_images)
                else:
                    # å•å¼ å›¾åƒï¼Œç¡®ä¿æ˜¯4D tensor (1, H, W, C)
                    output_batch = unpacked_images[0].unsqueeze(0) if unpacked_images[0].dim() == 3 else unpacked_images[0]
            else:
                # å¦‚æœæ²¡æœ‰å›¾åƒï¼Œè¿”å›é»‘è‰²å ä½å›¾
                output_batch = torch.zeros((1, 64, 64, 3), dtype=torch.float32)

            if processed_enable_preview:
                return {"ui": {"images": results}, "result": (output_batch,)}
            else:
                return {"ui": {"images": []}, "result": (output_batch,)}

        except Exception as e:
            debug_print(COMPONENT_NAME, f"[ImageCacheSave] â””â”€ âœ— ä¿å­˜å¤±è´¥: {str(e)}")
            import traceback
            debug_print(COMPONENT_NAME, traceback.format_exc())

            # å¼‚å¸¸æ—¶è¿”å›é»‘è‰²å ä½å›¾
            empty_batch = torch.zeros((1, 64, 64, 3), dtype=torch.float32)
            return {"ui": {"images": []}, "result": (empty_batch,)}


# èŠ‚ç‚¹æ˜ å°„
def get_node_class_mappings():
    return {
        "ImageCacheSave": ImageCache
    }

def get_node_display_name_mappings():
    return {
        "ImageCacheSave": "å›¾åƒç¼“å­˜ä¿å­˜ (Image Cache Save)"
    }

NODE_CLASS_MAPPINGS = get_node_class_mappings()
NODE_DISPLAY_NAME_MAPPINGS = get_node_display_name_mappings()