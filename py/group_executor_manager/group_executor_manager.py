"""
ç»„æ‰§è¡Œç®¡ç†å™¨ - ComfyUIèŠ‚ç‚¹
åŸºäºå®˜æ–¹æ–‡æ¡£çš„æ ‡å‡†æ ¼å¼å®ç°
"""

import json
import time
import uuid
import hashlib
import gc
from typing import Dict, Any, List

# å¯¼å…¥æ—¥å¿—ç³»ç»Ÿ
from ..utils.logger import get_logger

# åˆå§‹åŒ–logger
logger = get_logger(__name__)

# å¯¼å…¥å†…å­˜æ¸…ç†ç›¸å…³æ¨¡å—
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    logger.warning("torchä¸å¯ç”¨ï¼Œæ˜¾å­˜æ¸…ç†åŠŸèƒ½å°†è¢«ç¦ç”¨")

try:
    import comfy.model_management as mm
    COMFY_MM_AVAILABLE = True
except ImportError:
    COMFY_MM_AVAILABLE = False
    logger.warning("comfy.model_managementä¸å¯ç”¨ï¼Œæ¿€è¿›æ¨¡å¼æ¸…ç†å°†è¢«ç¦ç”¨")

# å¯¼å…¥é‡‡æ ·å™¨è¯†åˆ«åŠŸèƒ½ï¼ˆå·²ä» metadata_collector è¿ç§»åˆ° utils.configï¼‰
try:
    from ..utils.config import is_sampler_node
    SAMPLER_CHECK_AVAILABLE = True
except ImportError:
    SAMPLER_CHECK_AVAILABLE = False
    logger.warning("utils.configä¸å¯ç”¨ï¼Œé‡‡æ ·å™¨ç»„æ£€æµ‹å°†è¢«ç¦ç”¨")

    # æä¾›ä¸€ä¸ªfallbackå®ç°
    def is_sampler_node(class_type):
        """Fallback: ç®€å•çš„é‡‡æ ·å™¨èŠ‚ç‚¹åˆ¤æ–­"""
        return "sampler" in class_type.lower() or "ksampler" in class_type.lower()


class AnyType(str):
    """ç”¨äºè¡¨ç¤ºä»»æ„ç±»å‹çš„ç‰¹æ®Šç±»ï¼Œåœ¨ç±»å‹æ¯”è¾ƒæ—¶æ€»æ˜¯è¿”å›Falseï¼ˆä¸ç›¸ç­‰ï¼‰"""
    def __ne__(self, __value: object) -> bool:
        return False


any_typ = AnyType("*")


# âœ… å…¨å±€é…ç½®å­˜å‚¨ï¼ˆå‰åç«¯äº¤äº’çš„æ¢çº½ï¼‰
_group_executor_config = {
    "groups": [],
    "last_update": 0,
    "last_workflow_groups": []  # è¿½è¸ªå·¥ä½œæµä¸­çš„groups
}

def get_group_config():
    """è·å–å½“å‰ä¿å­˜çš„ç»„é…ç½®"""
    return _group_executor_config.get("groups", [])

def set_group_config(groups):
    """ä¿å­˜ç»„é…ç½®"""
    global _group_executor_config
    _group_executor_config["groups"] = groups
    _group_executor_config["last_update"] = time.time()
    logger.info(f"\n[GroupExecutorManager] âœ… é…ç½®å·²æ›´æ–°: {len(groups)} ä¸ªç»„")
    for i, group in enumerate(groups, 1):
        logger.debug(f"   {i}. {group.get('group_name', 'æœªå‘½å')}")


class GroupExecutorManager:
    """Basic Group Executor Manager"""

    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰è¾“å…¥å‚æ•°ç±»å‹ - çº¯è‡ªå®šä¹‰UIç‰ˆæœ¬"""
        return {
            "required": {},
            "optional": {},
            "hidden": {
                "unique_id": "UNIQUE_ID"
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("execution_data",)
    FUNCTION = "create_execution_plan"
    CATEGORY = "danbooru"
    DESCRIPTION = "ç»„æ‰§è¡Œç®¡ç†å™¨ï¼Œç”¨äºç®¡ç†å’Œæ§åˆ¶èŠ‚ç‚¹ç»„çš„æ‰§è¡Œé¡ºåºå’Œç¼“å­˜ç­–ç•¥"
    OUTPUT_IS_LIST = (False,)
    OUTPUT_NODE = True

    @classmethod
    def VALIDATE_INPUTS(cls, input_types):
        """è·³è¿‡wildcardç±»å‹çš„åç«¯éªŒè¯"""
        return True

    @classmethod
    def IS_CHANGED(cls, **kwargs) -> str:
        """
        åŸºäºé…ç½®å†…å®¹æ£€æµ‹ - åªæœ‰é…ç½®æ”¹å˜æ—¶æ‰é‡æ–°æ‰§è¡Œ

        å…³é”®ä¿®å¤ï¼š
        1. ä½¿ç”¨é…ç½®å†…å®¹çš„å“ˆå¸Œè€Œéæ—¶é—´æˆ³ï¼Œé¿å…æ¸…ç©ºä¾èµ–èŠ‚ç‚¹ï¼ˆå¦‚checkpointåŠ è½½å™¨ï¼‰çš„ç¼“å­˜
        2. åªæœ‰å½“ç”¨æˆ·ä¿®æ”¹ç»„é…ç½®æ—¶ï¼ŒIS_CHANGEDæ‰è¿”å›ä¸åŒçš„å€¼
        3. è¿™æ ·å¯ä»¥ä¿æŒcheckpointåŠ è½½å™¨ç­‰èŠ‚ç‚¹çš„ç¼“å­˜ï¼Œé¿å…æ¯æ¬¡æ‰§è¡Œéƒ½é‡æ–°åŠ è½½æ¨¡å‹ï¼ˆ8ç§’ï¼‰
        """
        # è·å–å½“å‰é…ç½®
        config_data = get_group_config()

        # åŸºäºé…ç½®å†…å®¹ç”Ÿæˆå“ˆå¸Œ
        # å°†é…ç½®åºåˆ—åŒ–ä¸ºç¨³å®šçš„å­—ç¬¦ä¸²ï¼ˆsortedç¡®ä¿é¡ºåºä¸€è‡´ï¼‰
        config_str = json.dumps(config_data, sort_keys=True, ensure_ascii=False)
        config_hash = hashlib.md5(config_str.encode()).hexdigest()

        return config_hash

    def create_execution_plan(self, unique_id=None):
        """
        åˆ›å»ºæ‰§è¡Œè®¡åˆ’

        Args:
            unique_id: èŠ‚ç‚¹çš„å”¯ä¸€ID

        Returns:
            tuple: (execution_data,) - åŒ…å«æ‰§è¡Œè®¡åˆ’å’Œç¼“å­˜æ§åˆ¶ä¿¡å·çš„JSONå­—ç¬¦ä¸²
        """
        try:
            logger.debug(f"\n{'='*80}")
            logger.debug(f"ğŸ¯ create_execution_plan è¢«è°ƒç”¨")
            logger.debug(f"{'='*80}")
            logger.debug(f"\n[GroupExecutorManager] ğŸ¯ å¼€å§‹ç”Ÿæˆæ‰§è¡Œè®¡åˆ’")
            logger.debug(f"ğŸ“ èŠ‚ç‚¹ID: {unique_id}")

            # âœ… ä»å…¨å±€é…ç½®ä¸­è¯»å–é…ç½®
            config_data = get_group_config()
            logger.debug(f"[GroupExecutorManager] ğŸ“¦ ä»å…¨å±€é…ç½®è¯»å–: {len(config_data)} ä¸ªç»„")

            # ğŸ” DEBUG: è¯¦ç»†è¾“å‡ºæ¯ä¸ªç»„çš„é…ç½®
            for i, group in enumerate(config_data, 1):
                logger.debug(f"ğŸ“¦ ç»„ {i}: {group.get('group_name', 'æœªå‘½å')}")
                cleanup_cfg = group.get('cleanup_config')
                if cleanup_cfg:
                    logger.info(f"âœ… cleanup_configå­˜åœ¨")
                    logger.debug(f"clear_vram: {cleanup_cfg.get('clear_vram')}")
                    logger.debug(f"clear_ram: {cleanup_cfg.get('clear_ram')}")
                    logger.debug(f"aggressive_mode: {cleanup_cfg.get('aggressive_mode')}")
                    logger.debug(f"delay_seconds: {cleanup_cfg.get('delay_seconds')}")
                else:
                    logger.error(f"âŒ cleanup_configä¸å­˜åœ¨æˆ–ä¸ºNone")

            # âœ… æ–°å¢ï¼šæ£€æµ‹é…ç½®æ˜¯å¦ä¸ºç©ºï¼Œå¦‚æœä¸ºç©ºåˆ™è¿”å›ç¦ç”¨çŠ¶æ€
            if not config_data or len(config_data) == 0:
                logger.warning(f"âš ï¸  é…ç½®ä¸ºç©ºï¼Œè¿”å›ç¦ç”¨çŠ¶æ€")
                disabled_data = {
                    "execution_plan": {
                        "disabled": True,
                        "disabled_reason": "empty_groups",
                        "message": "ç»„æ‰§è¡Œç®¡ç†å™¨é…ç½®ä¸ºç©ºï¼Œå·²è‡ªåŠ¨ç¦ç”¨",
                        "groups": [],
                        "execution_id": f"disabled_{int(time.time())}_{uuid.uuid4().hex[:8]}",
                        "execution_mode": "sequential",
                        "cache_control_mode": "conditional",
                        "client_id": None,
                        "cache_enabled": False,
                        "debug_mode": False
                    },
                    "cache_control_signal": {
                        "execution_id": f"disabled_{int(time.time())}_{uuid.uuid4().hex[:8]}",
                        "enabled": False,
                        "timestamp": time.time(),
                        "enable_cache": False,
                        "cache_key": "disabled",
                        "clear_cache": False,
                        "cache_control_mode": "conditional",
                        "disabled": True,
                        "disabled_reason": "empty_groups"
                    }
                }
                logger.info(f"ğŸš« å·²ç¦ç”¨ç»„æ‰§è¡ŒåŠŸèƒ½ï¼ˆåŸå› ï¼šé…ç½®ä¸ºç©ºï¼‰\n")
                return (json.dumps(disabled_data, ensure_ascii=False),)

            # âœ… æœ‰æœ‰æ•ˆé…ç½®ï¼Œç»§ç»­ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
            logger.info(f"âœ… ä½¿ç”¨ç”¨æˆ·é…ç½®çš„ç»„")

            # å›ºå®šé…ç½®å€¼ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
            execution_mode = "sequential"  # é¡ºåºæ‰§è¡Œ: sequential, å¹¶è¡Œæ‰§è¡Œ: parallel
            cache_control_mode = "conditional"  # æ¡ä»¶ç¼“å­˜: conditional, æ€»æ˜¯å…è®¸: always_allow, ç­‰å¾…è®¸å¯: block_until_allowed
            enable_cache = True
            debug_mode = False

            # âœ… æ¯æ¬¡æ‰§è¡Œéƒ½ç”Ÿæˆæ–°çš„execution_id
            execution_id = f"exec_{int(time.time())}_{uuid.uuid4().hex[:8]}"
            logger.info(f"âœ… ç”Ÿæˆæ–°çš„execution_id: {execution_id}")

            # åˆ›å»ºæ‰§è¡Œè®¡åˆ’ - åŒ…å«éªŒè¯å™¨éœ€è¦çš„æ‰€æœ‰å­—æ®µ
            execution_plan = {
                "groups": config_data,
                "execution_mode": execution_mode,  # âœ… ä¿®å¤ï¼šæ”¹ä¸º execution_mode
                "cache_control_mode": cache_control_mode,  # âœ… ä¿®å¤ï¼šæ·»åŠ  cache_control_mode
                "execution_id": execution_id,  # âœ… ä¿®å¤ï¼šç”Ÿæˆå”¯ä¸€ID
                "client_id": None,  # âœ… ä¿®å¤ï¼šç”±GroupExecutorTriggeråç«¯å¡«å……çœŸå®å€¼
                "cache_enabled": enable_cache,
                "debug_mode": debug_mode
            }

            # åˆ›å»ºç¼“å­˜æ§åˆ¶ä¿¡å·
            cache_signal = {
                "execution_id": execution_id,  # âœ… æ·»åŠ execution_idç”¨äºåŒ¹é…éªŒè¯
                "enabled": True,  # âœ… æƒé™æ£€æŸ¥éœ€è¦æ­¤å­—æ®µï¼Œè¡¨ç¤ºå…è®¸æ‰§è¡Œ
                "timestamp": time.time(),  # âœ… æ·»åŠ æ—¶é—´æˆ³ï¼Œé˜²æ­¢è¶…æ—¶æ£€æŸ¥å¤±è´¥
                "enable_cache": enable_cache,
                "cache_key": f"group_executor_{execution_mode}_{hash(str(config_data))}",
                "clear_cache": not enable_cache,
                "cache_control_mode": cache_control_mode  # âœ… æ·»åŠ ç¼“å­˜æ§åˆ¶æ¨¡å¼
            }

            # ğŸ“‹ è¯¦ç»†è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºç”Ÿæˆçš„æ‰§è¡Œè®¡åˆ’
            logger.debug(f"\n[GroupExecutorManager] ğŸ“‹ ç”Ÿæˆæ‰§è¡Œè®¡åˆ’è¯¦æƒ…:")
            logger.debug(f"   æ‰§è¡ŒID: {execution_id}")
            logger.debug(f"   ç»„æ•°é‡: {len(config_data)}")
            logger.debug(f"   æ‰§è¡Œæ¨¡å¼: {execution_mode}")
            logger.debug(f"   ç¼“å­˜æ¨¡å¼: {cache_control_mode}")
            logger.debug(f"   ")

            for i, group in enumerate(config_data, 1):
                group_name = group.get('group_name', f'æœªå‘½åç»„{i}')
                logger.debug(f"   â”œâ”€ ç»„{i}: {group_name}")

            logger.info(f"\n[GroupExecutorManager] âœ… æ‰§è¡Œè®¡åˆ’ç”Ÿæˆå®Œæˆ\n")

            # âœ… åˆå¹¶ä¸ºå•ä¸ªexecution_dataè¾“å‡º
            execution_data = {
                "execution_plan": execution_plan,
                "cache_control_signal": cache_signal
            }
            execution_data_json = json.dumps(execution_data, ensure_ascii=False)

            # ğŸ“¤ è¾“å‡ºæ—¥å¿—ï¼šæ˜¾ç¤ºå°†è¦å‘é€ç»™GroupExecutorTriggerçš„å†…å®¹
            logger.debug(f"ğŸ“¤ è¾“å‡ºå†…å®¹:")
            logger.debug(f"   â””â”€ execution_data (STRING):")
            logger.debug(f"      {execution_data_json[:200]}...")
            logger.debug(f"")

            return (execution_data_json,)

        except Exception as e:
            error_msg = f"GroupExecutorManager æ‰§è¡Œé”™è¯¯: {str(e)}"
            logger.error(f"\n[GroupExecutorManager] âŒ {error_msg}\n")
            import traceback
            traceback.print_exc()

            # è¿”å›é”™è¯¯ä¿¡æ¯
            error_data = {
                "execution_plan": {"error": error_msg, "execution_id": "error", "groups": []},
                "cache_control_signal": {"clear_cache": True, "error": True}
            }

            return (json.dumps(error_data, ensure_ascii=False),)

# èŠ‚ç‚¹æ˜ å°„ - ç”¨äºComfyUIæ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "GroupExecutorManager": GroupExecutorManager,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "GroupExecutorManager": "ç»„æ‰§è¡Œç®¡ç†å™¨ (Group Executor Manager)",
}

# âœ… æ·»åŠ APIç«¯ç‚¹ - ç”¨äºå‰åç«¯é…ç½®åŒæ­¥
try:
    from server import PromptServer
    from aiohttp import web
    
    routes = PromptServer.instance.routes
    
    @routes.post('/danbooru_gallery/group_config/save')
    async def save_group_config(request):
        """ä¿å­˜å‰ç«¯ä¼ å…¥çš„ç»„é…ç½®"""
        try:
            data = await request.json()
            groups = data.get('groups', [])

            logger.debug(f"\n[GroupExecutorManager API] ğŸ” ========== æ”¶åˆ°é…ç½®ä¿å­˜è¯·æ±‚ ==========")
            logger.debug(f"[GroupExecutorManager API] ğŸ“¦ ç»„æ•°é‡: {len(groups)}")

            # ğŸ” DEBUG: è¯¦ç»†è¾“å‡ºæ¯ä¸ªç»„çš„é…ç½®
            for i, group in enumerate(groups, 1):
                logger.debug(f"ğŸ“¦ ç»„ {i}: {group.get('group_name', 'æœªå‘½å')}")
                cleanup_cfg = group.get('cleanup_config')
                if cleanup_cfg:
                    logger.info(f"âœ… cleanup_configå­˜åœ¨:")
                    logger.debug(f"clear_vram: {cleanup_cfg.get('clear_vram')}")
                    logger.debug(f"clear_ram: {cleanup_cfg.get('clear_ram')}")
                    logger.debug(f"aggressive_mode: {cleanup_cfg.get('aggressive_mode')}")
                    logger.debug(f"delay_seconds: {cleanup_cfg.get('delay_seconds')}")
                else:
                    logger.error(f"âŒ cleanup_configä¸å­˜åœ¨æˆ–ä¸ºNone")

            # ä¿å­˜åˆ°å…¨å±€é…ç½®
            set_group_config(groups)

            logger.info(f"âœ… é…ç½®å·²ä¿å­˜åˆ°å…¨å±€å­˜å‚¨")
            logger.debug(f"========================================\n")

            return web.json_response({
                "status": "success",
                "message": f"å·²ä¿å­˜ {len(groups)} ä¸ªç»„çš„é…ç½®"
            })
        except Exception as e:
            error_msg = f"[GroupExecutorManager API] âŒ ä¿å­˜é…ç½®é”™è¯¯: {str(e)}"
            logger.debug(error_msg)
            logger.debug(f"========================================\n")
            import traceback
            traceback.print_exc()
            return web.json_response({
                "status": "error",
                "message": str(e)
            }, status=500)
    
    @routes.get('/danbooru_gallery/group_config/load')
    async def load_group_config(request):
        """è·å–å·²ä¿å­˜çš„ç»„é…ç½®"""
        try:
            groups = get_group_config()
            logger.info(f"\n[GroupExecutorManager API] ğŸ“¤ è¿”å›å·²ä¿å­˜çš„é…ç½®: {len(groups)} ä¸ªç»„")
            
            return web.json_response({
                "status": "success",
                "groups": groups,
                "last_update": _group_executor_config.get("last_update", 0)
            })
        except Exception as e:
            error_msg = f"[GroupExecutorManager API] è¯»å–é…ç½®é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            import traceback
            logger.debug(traceback.format_exc())
            return web.json_response({
                "status": "error",
                "message": str(e)
            }, status=500)

    @routes.post('/danbooru_gallery/group_executor/cleanup')
    async def perform_cleanup(request):
        """æ‰§è¡Œå†…å­˜/æ˜¾å­˜æ¸…ç†"""
        try:
            # è¯»å–è¯·æ±‚æ•°æ®
            data = await request.json()

            group_name = data.get('group_name', 'unknown')
            clear_vram = data.get('clear_vram', False)
            clear_ram = data.get('clear_ram', False)
            aggressive_mode = data.get('aggressive_mode', False)

            # æ¸…ç†å¼€å§‹é€šçŸ¥
            logger.debug(f"\n[æ¸…ç† API] ğŸš€ ========== å†…å­˜æ¸…ç†å¼€å§‹ ==========")
            logger.debug(f"ğŸ“ ç»„å: {group_name}")
            logger.debug(f"ğŸ“¦ æ¸…ç†é…ç½®:")
            logger.error(f"- VRAMæ¸…ç†: {'âœ… å¯ç”¨' if clear_vram else 'âŒ ç¦ç”¨'}")
            logger.error(f"- RAMæ¸…ç†: {'âœ… å¯ç”¨' if clear_ram else 'âŒ ç¦ç”¨'}")
            logger.error(f"- æ¿€è¿›æ¨¡å¼: {'âœ… å¯ç”¨' if aggressive_mode else 'âŒ ç¦ç”¨'}")

            # æ£€æŸ¥æ˜¯å¦å®é™…éœ€è¦æ¸…ç†
            if not clear_vram and not clear_ram:
                logger.info(f"â­ï¸ è·³è¿‡æ¸…ç†ï¼šæ‰€æœ‰é€‰é¡¹å‡å·²ç¦ç”¨")
                logger.debug(f"========================================\n")
                return web.json_response({
                    "status": "skipped",
                    "message": "è·³è¿‡æ¸…ç†ï¼šæœªå¯ç”¨ä»»ä½•é€‰é¡¹",
                    "results": {
                        "group_name": group_name,
                        "vram_cleaned": False,
                        "ram_cleaned": False,
                        "aggressive_used": False
                    }
                })

            results = {
                "group_name": group_name,
                "vram_cleaned": False,
                "ram_cleaned": False,
                "aggressive_used": False
            }

            # æ‰§è¡ŒVRAMæ¸…ç†
            if clear_vram:
                logger.debug(f"ğŸ”§ æ­£åœ¨æ‰§è¡Œ VRAM æ¸…ç†...")
                cleanup_vram()
                results["vram_cleaned"] = True
                logger.info(f"âœ… VRAM æ¸…ç†å®Œæˆ")

            # æ‰§è¡ŒRAMæ¸…ç†
            if clear_ram:
                mode_text = "æ¿€è¿›æ¨¡å¼" if aggressive_mode else "æ™®é€šæ¨¡å¼"
                logger.debug(f"ğŸ”§ æ­£åœ¨æ‰§è¡Œ RAM æ¸…ç†ï¼ˆ{mode_text}ï¼‰...")
                cleanup_ram(aggressive=aggressive_mode)
                results["ram_cleaned"] = True
                results["aggressive_used"] = aggressive_mode
                logger.info(f"âœ… RAM æ¸…ç†å®Œæˆï¼ˆ{mode_text}ï¼‰")

            # æ¸…ç†æ€»ç»“
            logger.debug(f"ğŸ“Š æ¸…ç†æ€»ç»“:")
            logger.info(f"- VRAM: {'âœ… å·²æ¸…ç†' if results['vram_cleaned'] else 'â­ï¸ è·³è¿‡'}")
            logger.info(f"- RAM: {'âœ… å·²æ¸…ç†' if results['ram_cleaned'] else 'â­ï¸ è·³è¿‡'}")
            if results['ram_cleaned']:
                logger.debug(f"- æ¨¡å¼: {'æ¿€è¿›æ¨¡å¼' if results['aggressive_used'] else 'æ™®é€šæ¨¡å¼'}")
            logger.debug(f"========================================\n")

            return web.json_response({
                "status": "success",
                "results": results
            })

        except Exception as e:
            error_msg = f"[æ¸…ç† API] âŒ å¼‚å¸¸: {str(e)}"
            logger.debug(error_msg)
            logger.debug(f"========================================\n")
            import traceback
            traceback.print_exc()
            return web.json_response({
                "status": "error",
                "message": str(e)
            }, status=500)

except ImportError as e:
    logger.warning(f"æ— æ³•å¯¼å…¥PromptServeræˆ–webæ¨¡å—ï¼ŒAPIç«¯ç‚¹å°†ä¸å¯ç”¨: {e}")


# ==================== å†…å­˜æ˜¾å­˜æ¸…ç†åŠŸèƒ½ ====================

def cleanup_vram():
    """æ¸…ç†æ˜¾å­˜ï¼ˆGPU VRAMï¼‰"""
    if not TORCH_AVAILABLE:
        logger.warning("âš ï¸ è·³è¿‡ï¼štorch æ¨¡å—ä¸å¯ç”¨")
        return

    try:
        if torch.cuda.is_available():
            logger.debug("[VRAMæ¸…ç†] ğŸ”§ æ‰§è¡Œ torch.cuda.empty_cache()")
            torch.cuda.empty_cache()
            logger.debug("[VRAMæ¸…ç†] ğŸ”§ æ‰§è¡Œ torch.cuda.ipc_collect()")
            torch.cuda.ipc_collect()
            logger.info("âœ… æ˜¾å­˜æ¸…ç†æˆåŠŸ")
        else:
            logger.warning("âš ï¸ è·³è¿‡ï¼šCUDA ä¸å¯ç”¨")
    except Exception as e:
        logger.error(f"âŒ æ¸…ç†å¤±è´¥: {e}")


def cleanup_ram(aggressive=False):
    """
    æ¸…ç†ç³»ç»Ÿå†…å­˜ï¼ˆRAMï¼‰

    Args:
        aggressive: æ˜¯å¦å¯ç”¨æ¿€è¿›æ¨¡å¼ï¼ˆå¸è½½æ‰€æœ‰æ¨¡å‹ï¼‰
    """
    try:
        # åŸºç¡€åƒåœ¾å›æ”¶
        logger.debug("[RAMæ¸…ç†] ğŸ”§ æ‰§è¡Œåƒåœ¾å›æ”¶ gc.collect()")
        gc.collect()

        if aggressive and COMFY_MM_AVAILABLE:
            # æ¿€è¿›æ¨¡å¼ï¼šå¸è½½æ‰€æœ‰æ¨¡å‹
            logger.debug("ğŸš€ æ¿€è¿›æ¨¡å¼ï¼šå¸è½½æ‰€æœ‰æ¨¡å‹")
            logger.debug("[RAMæ¸…ç†] ğŸ”§ æ‰§è¡Œ mm.unload_all_models()")
            mm.unload_all_models()
            logger.debug("[RAMæ¸…ç†] ğŸ”§ æ‰§è¡Œ mm.soft_empty_cache()")
            mm.soft_empty_cache()
            logger.info("âœ… å†…å­˜æ¸…ç†å®Œæˆï¼ˆæ¿€è¿›æ¨¡å¼ï¼‰")
        elif aggressive and not COMFY_MM_AVAILABLE:
            logger.warning("âš ï¸ æ¿€è¿›æ¨¡å¼ä¸å¯ç”¨ï¼ˆcomfy.model_management ä¸å¯ç”¨ï¼‰ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼")
            logger.info("âœ… å†…å­˜æ¸…ç†å®Œæˆï¼ˆæ™®é€šæ¨¡å¼ï¼‰")
        else:
            logger.info("âœ… å†…å­˜æ¸…ç†å®Œæˆï¼ˆæ™®é€šæ¨¡å¼ï¼‰")

    except Exception as e:
        logger.error(f"âŒ æ¸…ç†å¤±è´¥: {e}")


def has_next_sampler_group(current_index: int, groups: List[Dict], workflow: Dict) -> bool:
    """
    æ£€æµ‹åç»­æ˜¯å¦æœ‰åŒ…å«é‡‡æ ·å™¨çš„ç»„

    Args:
        current_index: å½“å‰ç»„çš„ç´¢å¼•
        groups: æ‰€æœ‰ç»„çš„åˆ—è¡¨
        workflow: å·¥ä½œæµæ•°æ®

    Returns:
        bool: å¦‚æœåç»­æœ‰é‡‡æ ·å™¨ç»„è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not SAMPLER_CHECK_AVAILABLE:
        logger.debug("è·³è¿‡é‡‡æ ·å™¨ç»„æ£€æµ‹ï¼šmetadata_collectorä¸å¯ç”¨")
        return False

    try:
        # éå†åç»­çš„ç»„
        for i in range(current_index + 1, len(groups)):
            group = groups[i]
            group_name = group.get("group_name")

            if not group_name:
                continue

            # è·å–å·¥ä½œæµä¸­è¯¥ç»„çš„æ‰€æœ‰èŠ‚ç‚¹
            if "nodes" not in workflow:
                continue

            for node_id, node_data in workflow["nodes"].items():
                # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦åœ¨è¯¥ç»„ä¸­
                if node_data.get("group") != group_name:
                    continue

                # æ£€æŸ¥æ˜¯å¦æ˜¯é‡‡æ ·å™¨èŠ‚ç‚¹
                class_type = node_data.get("class_type", "")
                if is_sampler_node(class_type):
                    logger.debug(f"[æ¡ä»¶è¯„ä¼°] æ£€æµ‹åˆ°åç»­é‡‡æ ·å™¨ç»„: {group_name} (èŠ‚ç‚¹: {node_id}, ç±»å‹: {class_type})")
                    return True

        logger.debug("åç»­æ— é‡‡æ ·å™¨ç»„")
        return False

    except Exception as e:
        logger.error(f"é‡‡æ ·å™¨ç»„æ£€æµ‹å¤±è´¥: {e}")
        return False


async def get_pcp_param_value(node_id: str, param_name: str) -> Any:
    """
    ä»å‚æ•°æ§åˆ¶é¢æ¿è·å–å‚æ•°å€¼

    Args:
        node_id: å‚æ•°èŠ‚ç‚¹ID
        param_name: å‚æ•°åç§°

    Returns:
        å‚æ•°å€¼ï¼Œå¦‚æœè·å–å¤±è´¥è¿”å›None
    """
    try:
        import aiohttp
        async with aiohttp.ClientSession() as session:
            url = f"http://127.0.0.1:8188/danbooru_gallery/pcp/get_param_value"
            params = {"node_id": node_id, "param_name": param_name}

            async with session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    if data.get("status") == "success":
                        value = data.get("value")
                        logger.debug(f"è·å–PCPå‚æ•°å€¼: {node_id}.{param_name} = {value}")
                        return value

        logger.error(f"è·å–PCPå‚æ•°å€¼å¤±è´¥: {node_id}.{param_name}")
        return None

    except Exception as e:
        logger.debug(f"è·å–PCPå‚æ•°å€¼å¼‚å¸¸: {e}")
        return None


async def evaluate_condition(condition: Dict, current_index: int, groups: List[Dict], workflow: Dict) -> bool:
    """
    è¯„ä¼°å•ä¸ªæ¡ä»¶

    Args:
        condition: æ¡ä»¶é…ç½®å­—å…¸
        current_index: å½“å‰ç»„ç´¢å¼•
        groups: æ‰€æœ‰ç»„åˆ—è¡¨
        workflow: å·¥ä½œæµæ•°æ®

    Returns:
        bool: æ¡ä»¶æ˜¯å¦æ»¡è¶³
    """
    try:
        condition_type = condition.get("type")
        expected_value = condition.get("value")

        if condition_type == "has_next_sampler_group":
            # æ£€æµ‹æ˜¯å¦æœ‰ä¸‹ä¸€ä¸ªé‡‡æ ·å™¨ç»„
            actual_value = has_next_sampler_group(current_index, groups, workflow)
            result = actual_value == expected_value
            logger.debug(f"has_next_sampler_group: {actual_value} == {expected_value} => {result}")
            return result

        elif condition_type == "pcp_param":
            # æ£€æµ‹å‚æ•°æ§åˆ¶é¢æ¿çš„å‚æ•°å€¼
            node_id = condition.get("node_id")
            param_name = condition.get("param_name")

            if not node_id or not param_name:
                logger.debug(f"pcp_paramæ¡ä»¶ç¼ºå°‘node_idæˆ–param_name")
                return False

            actual_value = await get_pcp_param_value(node_id, param_name)
            result = actual_value == expected_value
            logger.debug(f"pcp_param: {node_id}.{param_name} = {actual_value} == {expected_value} => {result}")
            return result

        else:
            logger.debug(f"æœªçŸ¥çš„æ¡ä»¶ç±»å‹: {condition_type}")
            return False

    except Exception as e:
        logger.debug(f"æ¡ä»¶è¯„ä¼°å¼‚å¸¸: {e}")
        return False


async def check_aggressive_conditions(conditions: List[Dict], current_index: int, groups: List[Dict], workflow: Dict) -> bool:
    """
    æ£€æŸ¥æ¿€è¿›æ¨¡å¼æ¡ä»¶ï¼ˆANDé€»è¾‘ï¼‰

    Args:
        conditions: æ¡ä»¶åˆ—è¡¨
        current_index: å½“å‰ç»„ç´¢å¼•
        groups: æ‰€æœ‰ç»„åˆ—è¡¨
        workflow: å·¥ä½œæµæ•°æ®

    Returns:
        bool: æ‰€æœ‰æ¡ä»¶éƒ½æ»¡è¶³è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not conditions:
        logger.debug("æ— æ¿€è¿›æ¨¡å¼æ¡ä»¶ï¼Œé»˜è®¤ä¸å¯ç”¨æ¿€è¿›æ¨¡å¼")
        return False

    try:
        logger.debug(f"[æ¡ä»¶è¯„ä¼°] å¼€å§‹è¯„ä¼° {len(conditions)} ä¸ªæ¿€è¿›æ¨¡å¼æ¡ä»¶")

        # è¯„ä¼°æ‰€æœ‰æ¡ä»¶ï¼ˆANDé€»è¾‘ï¼‰
        for i, condition in enumerate(conditions):
            result = await evaluate_condition(condition, current_index, groups, workflow)
            logger.debug(f"[æ¡ä»¶è¯„ä¼°] æ¡ä»¶ {i+1}/{len(conditions)}: {result}")

            if not result:
                logger.error(f"âŒ æ¡ä»¶ {i+1} ä¸æ»¡è¶³ï¼Œæ¿€è¿›æ¨¡å¼ä¸å¯ç”¨")
                return False

        logger.info("âœ… æ‰€æœ‰æ¡ä»¶éƒ½æ»¡è¶³ï¼Œå¯ç”¨æ¿€è¿›æ¨¡å¼")
        return True

    except Exception as e:
        logger.debug(f"æ¡ä»¶æ£€æŸ¥å¼‚å¸¸: {e}")
        return False