"""
å“ˆå¸Œç¼“å­˜ç®¡ç†å™¨ - ç”¨äºåŠ é€Ÿæ¨¡å‹æ–‡ä»¶å“ˆå¸Œè®¡ç®—

åŠŸèƒ½ï¼š
- å†…å­˜LRUç¼“å­˜ï¼ˆåŸºäºæ–‡ä»¶è·¯å¾„+ä¿®æ”¹æ—¶é—´ï¼‰
- JSONæŒä¹…åŒ–å­˜å‚¨
- çº¿ç¨‹å®‰å…¨æ“ä½œ
- è‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜
"""

import os
import json
import hashlib
import threading
from typing import Dict, Optional, Tuple
from pathlib import Path
from collections import OrderedDict
from ..utils.logger import get_logger

# åˆå§‹åŒ–logger
logger = get_logger(__name__)


class HashCacheManager:
    """å“ˆå¸Œç¼“å­˜ç®¡ç†å™¨ - æä¾›é«˜é€Ÿç¼“å­˜å’ŒæŒä¹…åŒ–æ”¯æŒ"""

    # ç±»çº§åˆ«çš„å•ä¾‹å®ä¾‹å’Œé”
    _instance = None
    _instance_lock = threading.Lock()

    def __new__(cls, cache_file: str = None, max_memory_entries: int = 100):
        """å•ä¾‹æ¨¡å¼ï¼Œç¡®ä¿å…¨å±€åªæœ‰ä¸€ä¸ªç¼“å­˜å®ä¾‹"""
        if cls._instance is None:
            with cls._instance_lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, cache_file: str = None, max_memory_entries: int = 100):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨

        Args:
            cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•ä¸‹çš„ hash_cache.json
            max_memory_entries: å†…å­˜ç¼“å­˜æœ€å¤§æ¡ç›®æ•°ï¼ˆLRUï¼‰
        """
        # é¿å…é‡å¤åˆå§‹åŒ–
        if hasattr(self, '_initialized'):
            return

        self._initialized = True
        self.max_memory_entries = max_memory_entries
        self._cache_lock = threading.Lock()

        # è®¾ç½®ç¼“å­˜æ–‡ä»¶è·¯å¾„
        if cache_file is None:
            current_dir = Path(__file__).parent
            cache_file = current_dir / "hash_cache.json"
        self.cache_file = Path(cache_file)

        # å†…å­˜ç¼“å­˜ï¼š{(file_path, mtime): hash_value}
        self._memory_cache: OrderedDict[Tuple[str, float], str] = OrderedDict()

        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            'hits': 0,
            'misses': 0,
            'disk_loads': 0,
            'disk_saves': 0
        }

        # ä»ç£ç›˜åŠ è½½ç¼“å­˜
        self._load_cache_from_disk()

    def _load_cache_from_disk(self):
        """ä»JSONæ–‡ä»¶åŠ è½½ç¼“å­˜"""
        if not self.cache_file.exists():
            return

        try:
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                disk_cache = json.load(f)

            # å°†ç£ç›˜ç¼“å­˜åŠ è½½åˆ°å†…å­˜ï¼Œä½†ä¸è¶…è¿‡æœ€å¤§æ¡ç›®æ•°
            loaded_count = 0
            for file_path, cache_data in disk_cache.items():
                if loaded_count >= self.max_memory_entries:
                    break

                # éªŒè¯æ–‡ä»¶æ˜¯å¦ä»å­˜åœ¨ä¸”ä¿®æ”¹æ—¶é—´åŒ¹é…
                if os.path.exists(file_path):
                    cached_mtime = cache_data.get('mtime')
                    cached_hash = cache_data.get('hash')

                    if cached_mtime and cached_hash:
                        current_mtime = os.path.getmtime(file_path)
                        # å¦‚æœæ–‡ä»¶æœªä¿®æ”¹ï¼ŒåŠ è½½åˆ°å†…å­˜
                        if abs(current_mtime - cached_mtime) < 1.0:  # å…è®¸1ç§’è¯¯å·®
                            cache_key = (file_path, current_mtime)
                            self._memory_cache[cache_key] = cached_hash
                            loaded_count += 1

            self._stats['disk_loads'] += 1
            logger.info(f"ä»ç£ç›˜åŠ è½½äº† {loaded_count} æ¡ç¼“å­˜è®°å½•")

        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

    def _save_cache_to_disk(self):
        """ä¿å­˜ç¼“å­˜åˆ°JSONæ–‡ä»¶"""
        try:
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            disk_cache = {}
            for (file_path, mtime), hash_value in self._memory_cache.items():
                disk_cache[file_path] = {
                    'mtime': mtime,
                    'hash': hash_value
                }

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            self.cache_file.parent.mkdir(parents=True, exist_ok=True)

            # å†™å…¥æ–‡ä»¶
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(disk_cache, f, indent=2, ensure_ascii=False)

            self._stats['disk_saves'] += 1

        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

    def get_hash(self, file_path: str) -> Optional[str]:
        """
        è·å–æ–‡ä»¶å“ˆå¸Œå€¼ï¼ˆä¼˜å…ˆä»ç¼“å­˜ï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            å“ˆå¸Œå€¼ï¼Œå¦‚æœç¼“å­˜æœªå‘½ä¸­åˆ™è¿”å› None
        """
        if not os.path.exists(file_path):
            return None

        # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        mtime = os.path.getmtime(file_path)
        cache_key = (file_path, mtime)

        with self._cache_lock:
            # æ£€æŸ¥å†…å­˜ç¼“å­˜
            if cache_key in self._memory_cache:
                # LRU: ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
                self._memory_cache.move_to_end(cache_key)
                self._stats['hits'] += 1
                return self._memory_cache[cache_key]

            # æ£€æŸ¥æ˜¯å¦æœ‰è¯¥æ–‡ä»¶çš„æ—§ç¼“å­˜ï¼ˆä¸åŒmtimeï¼‰
            for (cached_path, cached_mtime), cached_hash in list(self._memory_cache.items()):
                if cached_path == file_path and cached_mtime != mtime:
                    # æ–‡ä»¶å·²ä¿®æ”¹ï¼Œåˆ é™¤æ—§ç¼“å­˜
                    del self._memory_cache[(cached_path, cached_mtime)]

            self._stats['misses'] += 1
            return None

    def set_hash(self, file_path: str, hash_value: str, auto_save: bool = True):
        """
        è®¾ç½®æ–‡ä»¶å“ˆå¸Œå€¼åˆ°ç¼“å­˜

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            hash_value: å“ˆå¸Œå€¼
            auto_save: æ˜¯å¦è‡ªåŠ¨ä¿å­˜åˆ°ç£ç›˜
        """
        if not os.path.exists(file_path):
            return

        mtime = os.path.getmtime(file_path)
        cache_key = (file_path, mtime)

        with self._cache_lock:
            # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆLRUï¼‰
            if len(self._memory_cache) >= self.max_memory_entries:
                self._memory_cache.popitem(last=False)

            # æ·»åŠ åˆ°ç¼“å­˜
            self._memory_cache[cache_key] = hash_value

            # è‡ªåŠ¨ä¿å­˜åˆ°ç£ç›˜
            if auto_save:
                self._save_cache_to_disk()

    def calculate_and_cache_hash(self, file_path: str, block_size: int = 128 * 1024) -> str:
        """
        è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¹¶ç¼“å­˜

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            block_size: è¯»å–å—å¤§å°ï¼ˆå­—èŠ‚ï¼‰

        Returns:
            SHA256å“ˆå¸Œå€¼ï¼ˆå‰10ä½ï¼Œå°å†™ï¼‰
        """
        # å…ˆå°è¯•ä»ç¼“å­˜è·å–
        cached_hash = self.get_hash(file_path)
        if cached_hash:
            return cached_hash

        # è®¡ç®—å“ˆå¸Œ
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(block_size), b""):
                sha256_hash.update(byte_block)

        hash_result = sha256_hash.hexdigest()[:10].lower()

        # ä¿å­˜åˆ°ç¼“å­˜
        self.set_hash(file_path, hash_result)

        return hash_result

    def clear_cache(self, save_to_disk: bool = True):
        """
        æ¸…ç©ºç¼“å­˜

        Args:
            save_to_disk: æ¸…ç©ºå‰æ˜¯å¦ä¿å­˜åˆ°ç£ç›˜
        """
        with self._cache_lock:
            if save_to_disk:
                self._save_cache_to_disk()

            self._memory_cache.clear()
            logger.info("ç¼“å­˜å·²æ¸…ç©º")

    def get_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self._cache_lock:
            total_requests = self._stats['hits'] + self._stats['misses']
            hit_rate = (self._stats['hits'] / total_requests * 100) if total_requests > 0 else 0

            return {
                'memory_entries': len(self._memory_cache),
                'max_entries': self.max_memory_entries,
                'hits': self._stats['hits'],
                'misses': self._stats['misses'],
                'hit_rate': f"{hit_rate:.2f}%",
                'disk_loads': self._stats['disk_loads'],
                'disk_saves': self._stats['disk_saves']
            }

    def print_stats(self):
        """æ‰“å°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ“Š å“ˆå¸Œç¼“å­˜ç»Ÿè®¡ä¿¡æ¯")
        logger.info("=" * 50)
        logger.info(f"å†…å­˜ç¼“å­˜æ¡ç›®: {stats['memory_entries']} / {stats['max_entries']}")
        logger.info(f"ç¼“å­˜å‘½ä¸­: {stats['hits']} æ¬¡")
        logger.info(f"ç¼“å­˜æœªå‘½ä¸­: {stats['misses']} æ¬¡")
        logger.info(f"å‘½ä¸­ç‡: {stats['hit_rate']}")
        logger.info(f"ç£ç›˜åŠ è½½: {stats['disk_loads']} æ¬¡")
        logger.info(f"ç£ç›˜ä¿å­˜: {stats['disk_saves']} æ¬¡")
        logger.info("=" * 50 + "\n")

    def remove_file_cache(self, file_path: str):
        """
        åˆ é™¤æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰ç¼“å­˜

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        """
        with self._cache_lock:
            keys_to_remove = [
                key for key in self._memory_cache.keys()
                if key[0] == file_path
            ]
            for key in keys_to_remove:
                del self._memory_cache[key]

    def force_save(self):
        """å¼ºåˆ¶ä¿å­˜å½“å‰ç¼“å­˜åˆ°ç£ç›˜"""
        with self._cache_lock:
            self._save_cache_to_disk()
            logger.info("ç¼“å­˜å·²å¼ºåˆ¶ä¿å­˜åˆ°ç£ç›˜")


# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
_global_cache_manager = None
_global_cache_lock = threading.Lock()


def get_cache_manager() -> HashCacheManager:
    """è·å–å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹"""
    global _global_cache_manager

    if _global_cache_manager is None:
        with _global_cache_lock:
            if _global_cache_manager is None:
                _global_cache_manager = HashCacheManager()

    return _global_cache_manager
